from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import json
from get_combined_scores import algo_scores

# Load the JSON data
with open("normalized_scores_new.json", "r") as file:
    data = json.load(file)

algos = ['MV', 'Hishigaki', 'RWR', 'FF', 'Ensemble']

precision_recall_data = {}

for algo in algos:
    precision_recall_data[algo] = {}

# Store precision-recall values
for algo in algos:
    true_labels = []
    predicted_labels = []
    # for each fold get predicted labels and true labels
    for i in range(1, 11):
        ps = data[str(i)]['test_seeds']
        for p in ps:
            true_labels.append(1)
            if algo == "Hishigaki":
                predicted_labels.append(algo_scores['Hishi'][p])
            else:
                predicted_labels.append(algo_scores[algo][p])

        ns = list(set(data[str(i)]['MV']) - set(ps))
        for n in ns:
            true_labels.append(0)
            if algo == "Hishigaki":
                predicted_labels.append(algo_scores['Hishi'][n])
            else:
                predicted_labels.append(algo_scores[algo][n])

    # calculating metrics
    precision, recall, _ = precision_recall_curve(true_labels, predicted_labels)
    roc_auc = roc_auc_score(true_labels, predicted_labels)
    fpr, tpr, thresholds = roc_curve(true_labels, predicted_labels)
    auc_pr = auc(recall, precision)
    print('AUPR for', algo, 'is', auc_pr)
    print('AUROC for', algo, 'is', roc_auc)

    # store the precision recall values of all algorithms
    precision_recall_data[algo] = {'precision': list(precision), 'recall': list(recall), 'fpr': list(fpr),
                                   'tpr': list(tpr), 'AUPR': auc_pr, 'AUROC': roc_auc}
    with open('precision_recall_data.json', 'w') as file:
        json.dump(precision_recall_data, file, indent=4)

# Plot the Precision-Recall curve for all algorithms
plt.figure(figsize=(10, 8))

colors = {"MV": "blue", "Hishigaki": "red", "RWR": "green", "FF": "purple", "Ensemble": "brown"}
for algo, color in colors.items():
    precision_vals = precision_recall_data[algo]['precision']
    recall_vals = precision_recall_data[algo]['recall']
    plt.plot(recall_vals, precision_vals, color=color, alpha=0.5, label=f'{algo} Precision-Recall')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend(loc='best')
plt.show()

# Plot ROC curves
for algo, color in colors.items():
    fpr_vals = precision_recall_data[algo]['fpr']
    tpr_vals = precision_recall_data[algo]['tpr']
    plt.plot(fpr_vals, tpr_vals, color=color, alpha=0.5, label=f'{algo} ROC Curve')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')  # Diagonal line (Random classifier)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()
